#'  Fitting Multinomial Logistic Regression model in "Divide and Recombine" approach to Large Data Sets
#' @description
#' Function \code{drglm.multinom} fits multinomial logistic regressiosn model to big data sets in divide and recombine approach.
#' @param formula An entity belonging to the "formula" class (or one that can be transformed into that class) represents a symbolic representation of the model that needs to be adjusted. Specifics about how the model is defined can be found in the 'Details' section.
#' @param data A data frame, list, or environment that is not required but can be provided if available.
#' @param k Number of subsets to be used.

#' @return A "Multinomial (Polytomous) Logistic Regression  Model" is fitted in "Divide and Recombine" approach.
#' @references
#' Karim, M. R., & Islam, M. A. (2019). Reliability and Survival Analysis. In Reliability and Survival Analysis.
#' Venables WN, Ripley BD (2002). Modern Applied Statistics with S, Fourth edition. Springer, New York. ISBN 0-387-95457-0, https://www.stats.ox.ac.uk/pub/MASS4/.
#' @author MH Nayem
#' @seealso \code{\link{big.drglm}}, \code{\link{drglm}}
#' @import nnet
#' @importFrom nnet multinom
#' @import stats
#' @importFrom stats glm coef qnorm pnorm
#' @export drglm.multinom
#' @examples
#' set.seed(123)
#' #Number of rows to be generated
#' n <- 10000
#' #creating dataset
#' dataset <- data.frame( pred_1 = round(rnorm(n, mean = 50, sd = 10)),
#' pred_2 = round(rnorm(n, mean = 7.5, sd = 2.1)),
#' pred_3 = as.factor(sample(c("0", "1"), n, replace = TRUE)),
#' pred_4 = as.factor(sample(c("0", "1", "2"), n, replace = TRUE)),
#' pred_5 = as.factor(sample(0:15, n, replace = TRUE)),
#' pred_6 = round(rnorm(n, mean = 60, sd = 5)))

#' #fitting multinomial logistic regression model
#' mmodel=drglm::drglm.multinom(
#' pred_4~ pred_1+ pred_2+ pred_3+ pred_5+ pred_6, data=dataset, k=10)
#' #Output
#' mmodel

drglm.multinom<-function(formula,data,k)
{
  n = nrow(data)
  rows_per_chunk = ceiling(n / k)
  split_data = lapply(1:k, function(i) {
    start_row = (i-1) * rows_per_chunk + 1
    end_row = min(i * rows_per_chunk, n)
    return(data[start_row:end_row, ])
  })


  # creating models
  model <- list()

  # loop over the split_data list
  for (i in 1:length(split_data)) {

    model[[i]] <- nnet::multinom(formula=formula,data=split_data[[i]],Hess = TRUE)
  }
  response=as.character(formula(model[[1]])[[2]])
  b <- list()
  for (i in 1:length(model))
  {
    b[[i]] <- as.matrix(coef(model[[i]]))
  }

  B={Reduce("+",b)}/k


  H<-list()
  for (i in 1:length(model))
  {
    H[[i]]<-(model[[i]])$Hessian
  }

  vcov <- list()

  for (i in 1:length(model)) {

    vcov[[i]] <- as.matrix(solve(H[[i]]))
  }

  v <- list()
  for (i in 1:length(vcov))
  {
    v[[i]] <- matrix(diag(vcov[[i]]),nrow=(nlevels(data[[response]]))-1,byrow=TRUE)/k
  }

  v_com <- Reduce("+", v)/k

  se_com <- sqrt(v_com)

  alpha <- 0.05

  z<-stats::qnorm(1-alpha/2)


  #lower and upper bounds using gaussian distribution
  l_normal= B-z*se_com
  u_normal=B+z*se_com

  Z=B/se_com

  p_value=2*(1- stats::pnorm(abs(Z)))

  table <- data.frame("Estimate"=t(B),
                      "standard error"=t(se_com) ,
                      "z value"= t(Z),
                      "Pr(>|z|)"=t(p_value),
                      "95% lower CI"=t(l_normal),
                      "95% upper CI" = t(u_normal ),
                      check.names = FALSE)

  # viewtable
  table
}
